{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD - SR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook allows to replicate the simulation as describe in Alamia et al. (2019). CNN and a Siamese networks are compared when performing identity (SD) and Spatial Relationship (SR) tasks. See Alamia et al. (2019) for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having imported a bunch of packages, we load the data from a previously genereated Matlab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "import csv\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import MaxPooling2D, Conv2D, ZeroPadding2D, Activation, Input, concatenate, Add, Multiply\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1234)\n",
    "\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.layers.merge import _Merge\n",
    "\n",
    "import scipy.io as sio\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getImagesNlabelsSiam() :\n",
    "\n",
    "    infile = sio.loadmat('siamese1000val.mat')\n",
    "    \n",
    "    allTrialsTr = infile.get('allTrialsTr')\n",
    "    SRconditionsTr=infile.get('SRconditionsTr')\n",
    "    SDconditionsTr=infile.get('SDconditionsTr')\n",
    "    \n",
    "    allTrialsVal = infile.get('allTrialsVal')\n",
    "    SRconditionsVal=infile.get('SRconditionsVal')\n",
    "    SDconditionsVal=infile.get('SDconditionsVal')\n",
    "    \n",
    "    allTrialsTe = infile.get('allTrialsTe')\n",
    "    SRconditionsTe=infile.get('SRconditionsTe')\n",
    "    SDconditionsTe=infile.get('SDconditionsTe')\n",
    "\n",
    "    #shuffling training\n",
    "    perm = np.arange(allTrialsTr.shape[3])  \n",
    "    np.random.shuffle(perm)  \n",
    "    allTrialsTrPerm = allTrialsTr[:,:,:,perm] \n",
    "    SRconditionsTrPerm = SRconditionsTr[:,perm] \n",
    "    SDconditionsTrPerm = SDconditionsTr[:,perm] \n",
    "   \n",
    "    #providing it with the right dimensions  training\n",
    "    allTrialsTrRearranged=np.moveaxis(allTrialsTrPerm,-1,0)\n",
    "    SDconditionsTrRearranged=np.moveaxis(SDconditionsTrPerm,-1,0)\n",
    "    SRconditionsTrRearranged=np.moveaxis(SRconditionsTrPerm,-1,0)\n",
    "    allTrialsTrRearranged2 = np.expand_dims(allTrialsTrRearranged,axis=-1)\n",
    "    allTrialsTrRearranged2 = allTrialsTrRearranged\n",
    "    \n",
    "    \n",
    "    #shuffling validation\n",
    "    perm = np.arange(allTrialsVal.shape[3])  \n",
    "    np.random.shuffle(perm)  \n",
    "    allTrialsValPerm = allTrialsVal[:,:,:,perm] \n",
    "    SRconditionsValPerm = SRconditionsVal[:,perm] \n",
    "    SDconditionsValPerm = SDconditionsVal[:,perm] \n",
    "   \n",
    "    #providing it with the right dimensions validation\n",
    "    allTrialsValRearranged=np.moveaxis(allTrialsValPerm,-1,0)\n",
    "    SDconditionsValRearranged=np.moveaxis(SDconditionsValPerm,-1,0)\n",
    "    SRconditionsValRearranged=np.moveaxis(SRconditionsValPerm,-1,0)\n",
    "    allTrialsValRearranged2 = np.expand_dims(allTrialsValRearranged,axis=-1)\n",
    "    allTrialsValRearranged2 = allTrialsValRearranged\n",
    "    \n",
    "    \n",
    "    #shuffling testing\n",
    "    perm = np.arange(allTrialsTe.shape[3])  \n",
    "    np.random.shuffle(perm)  \n",
    "    allTrialsTePerm = allTrialsTe[:,:,:,perm] \n",
    "    SRconditionsTePerm = SRconditionsTe[:,perm] \n",
    "    SDconditionsTePerm = SDconditionsTe[:,perm] \n",
    "   \n",
    "    #providing it with the right dimensions  testing\n",
    "    allTrialsTeRearranged=np.moveaxis(allTrialsTePerm,-1,0)\n",
    "    SDconditionsTeRearranged=np.moveaxis(SDconditionsTePerm,-1,0)\n",
    "    SRconditionsTeRearranged=np.moveaxis(SRconditionsTePerm,-1,0)\n",
    "    allTrialsTeRearranged2 = np.expand_dims(allTrialsTeRearranged,axis=-1)\n",
    "    allTrialsTeRearranged2 = allTrialsTeRearranged\n",
    "    \n",
    "    return {'imagesTr':allTrialsTrRearranged2,'SRlabelsTr':SRconditionsTrRearranged,'SDlabelsTr':SDconditionsTrRearranged,'imagesVal':allTrialsValRearranged2,'SRlabelsVal':SRconditionsValRearranged,'SDlabelsVal':SDconditionsValRearranged,'imagesTe':allTrialsTeRearranged2,'SRlabelsTe':SRconditionsTeRearranged,'SDlabelsTe':SDconditionsTeRearranged}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getImagesNlabels() :\n",
    "\n",
    "    #infile = sio.loadmat('artVisDataset.mat')\n",
    "    infile = sio.loadmat('cnn1000val.mat')\n",
    "\n",
    "    allTrialsTr = infile.get('allTrialsTr')\n",
    "    SRconditionsTr=infile.get('SRconditionsTr')\n",
    "    SDconditionsTr=infile.get('SDconditionsTr')\n",
    "    \n",
    "    allTrialsVal = infile.get('allTrialsVal')\n",
    "    SRconditionsVal=infile.get('SRconditionsVal')\n",
    "    SDconditionsVal=infile.get('SDconditionsVal')\n",
    "    \n",
    "    allTrialsTe = infile.get('allTrialsTe')\n",
    "    SRconditionsTe=infile.get('SRconditionsTe')\n",
    "    SDconditionsTe=infile.get('SDconditionsTe')\n",
    "    \n",
    "\n",
    "    #shuffling training\n",
    "    perm = np.arange(allTrialsTr.shape[2])  \n",
    "    np.random.shuffle(perm)  \n",
    "    allTrialsTrPerm = allTrialsTr[:,:,perm] \n",
    "    SRconditionsTrPerm = SRconditionsTr[:,perm] \n",
    "    SDconditionsTrPerm = SDconditionsTr[:,perm] \n",
    "   \n",
    "    #providing it with the right dimensions  training\n",
    "    allTrialsTrRearranged=np.moveaxis(allTrialsTrPerm,-1,0)\n",
    "    SDconditionsTrRearranged=np.moveaxis(SDconditionsTrPerm,-1,0)\n",
    "    SRconditionsTrRearranged=np.moveaxis(SRconditionsTrPerm,-1,0)\n",
    "    allTrialsTrRearranged2 = np.expand_dims(allTrialsTrRearranged,axis=-1)\n",
    "    \n",
    "        \n",
    "    #shuffling validation\n",
    "    perm = np.arange(allTrialsVal.shape[2])  \n",
    "    np.random.shuffle(perm)  \n",
    "    allTrialsValPerm = allTrialsVal[:,:,perm] \n",
    "    SRconditionsValPerm = SRconditionsVal[:,perm] \n",
    "    SDconditionsValPerm = SDconditionsVal[:,perm] \n",
    "   \n",
    "    #providing it with the right dimensions validation\n",
    "    allTrialsValRearranged=np.moveaxis(allTrialsValPerm,-1,0)\n",
    "    SDconditionsValRearranged=np.moveaxis(SDconditionsValPerm,-1,0)\n",
    "    SRconditionsValRearranged=np.moveaxis(SRconditionsValPerm,-1,0)\n",
    "    allTrialsValRearranged2 = np.expand_dims(allTrialsValRearranged,axis=-1)\n",
    "    allTrialsValRearranged2 = allTrialsValRearranged\n",
    "    \n",
    "    \n",
    "    #shuffling testing\n",
    "    perm = np.arange(allTrialsTe.shape[2])  \n",
    "    np.random.shuffle(perm)  \n",
    "    allTrialsTePerm = allTrialsTe[:,:,perm] \n",
    "    SRconditionsTePerm = SRconditionsTe[:,perm] \n",
    "    SDconditionsTePerm = SDconditionsTe[:,perm] \n",
    "   \n",
    "    #providing it with the right dimensions  testing\n",
    "    allTrialsTeRearranged=np.moveaxis(allTrialsTePerm,-1,0)\n",
    "    SDconditionsTeRearranged=np.moveaxis(SDconditionsTePerm,-1,0)\n",
    "    SRconditionsTeRearranged=np.moveaxis(SRconditionsTePerm,-1,0)\n",
    "    allTrialsTeRearranged2 = np.expand_dims(allTrialsTeRearranged,axis=-1)\n",
    "    \n",
    "    return {'imagesTr':allTrialsTrRearranged2,'SRlabelsTr':SRconditionsTrRearranged,'SDlabelsTr':SDconditionsTrRearranged,'imagesTe':allTrialsTeRearranged2,'SRlabelsTe':SRconditionsTeRearranged,'SDlabelsTe':SDconditionsTeRearranged}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells we define the Siamese Network and the convolutional one. Note that in the Siamese network we redefine the class Substract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Subtract(_Merge):\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Subtract, self).build(input_shape)\n",
    "        if len(input_shape) != 2:\n",
    "            raise ValueError('A `Subtract` layer should be called '\n",
    "                             'on exactly 2 inputs')\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        if len(inputs) != 2:\n",
    "            raise ValueError('A `Subtract` layer should be called '\n",
    "                             'on exactly 2 inputs')\n",
    "        return inputs[0] - inputs[1]\n",
    "\n",
    "\n",
    "def get_siamese_model(images):\n",
    "\n",
    "    # Define the tensors for the two input images\n",
    "#     left_input = Input(images[:,:,:,0].shape)\n",
    "#     right_input = Input(images[:,:,:,1].shape)\n",
    "#     left_input = np.expand_dims(left_input,axis=-1)\n",
    "#     right_input = np.expand_dims(right_input,axis=-1)\n",
    "#     left_input = Input(left_input.shape)\n",
    "#     right_input = Input(right_input.shape)\n",
    "    \n",
    "    input_shape = (images.shape[1], images.shape[2], 1)\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(4, (2, 2), strides=1, input_shape=(images.shape[1], images.shape[2], 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv2D(4, (2, 2), strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(4, (2, 2), strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))   \n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(4, (2, 2), strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv2D(4, (2, 2), strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv2D(4, (2, 2), strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    combineFeatures2 = Subtract()([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction1 = Dense(128,activation='relu')(combineFeatures2)\n",
    "    prediction1do = Dropout(.3)(prediction1)\n",
    "    prediction2 = Dense(128,activation='relu')(prediction1)\n",
    "    prediction2do = Dropout(.3)(prediction2)\n",
    "    prediction = Dense(1,activation='sigmoid')(prediction2do)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    siamese_net.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=optimizers.Adam(lr=0.0001))\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildingTheNetwork(images) :\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(4, (2, 2), strides=1, input_shape=(images.shape[1], images.shape[2], 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(4, (2, 2), strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(4, (2, 2), strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))   \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(4, (2, 2), strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(4, (2, 2), strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(4, (2, 2), strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(4, (2, 2), strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=optimizers.Adam(lr=0.0001),)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we finally run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'K' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f983018319a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetImagesNlabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimagesTr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'imagesTr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mSRlabelsTr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SRlabelsTr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'K' is not defined"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "data=getImagesNlabels() \n",
    "imagesTr=data['imagesTr']\n",
    "SRlabelsTr=data['SRlabelsTr']\n",
    "SDlabelsTr=data['SDlabelsTr']\n",
    "\n",
    "imagesTe=data['imagesTe']\n",
    "SRlabelsTe=data['SRlabelsTe']\n",
    "SDlabelsTe=data['SDlabelsTe']\n",
    "\n",
    "\n",
    "dataSiam=getImagesNlabelsSiam() \n",
    "imagesTrSiam=dataSiam['imagesTr']\n",
    "SRlabelsTrSiam=dataSiam['SRlabelsTr']\n",
    "SDlabelsTrSiam=dataSiam['SDlabelsTr']\n",
    "\n",
    "imagesValSiam=dataSiam['imagesVal']\n",
    "SRlabelsValSiam=dataSiam['SRlabelsVal']\n",
    "SDlabelsValSiam=dataSiam['SDlabelsVal']\n",
    "\n",
    "imagesTeSiam=dataSiam['imagesTe']\n",
    "SRlabelsTeSiam=dataSiam['SRlabelsTe']\n",
    "SDlabelsTeSiam=dataSiam['SDlabelsTe']\n",
    "\n",
    "\n",
    "imagesTrNewL=imagesTrSiam[:,:,:,0]\n",
    "imagesTrNewR=imagesTrSiam[:,:,:,1]\n",
    "imagesTrNewL = np.expand_dims(imagesTrNewL,axis=-1)\n",
    "imagesTrNewR = np.expand_dims(imagesTrNewR,axis=-1)\n",
    "\n",
    "imagesValNewL=imagesValSiam[:,:,:,0]\n",
    "imagesValNewR=imagesValSiam[:,:,:,1]\n",
    "imagesValNewL = np.expand_dims(imagesValNewL,axis=-1)\n",
    "imagesValNewR = np.expand_dims(imagesValNewR,axis=-1)\n",
    "\n",
    "imagesTeNewL=imagesTeSiam[:,:,:,0]\n",
    "imagesTeNewR=imagesTeSiam[:,:,:,1]\n",
    "imagesTeNewL = np.expand_dims(imagesTeNewL,axis=-1)\n",
    "imagesTeNewR = np.expand_dims(imagesTeNewR,axis=-1)\n",
    "\n",
    "networkSR=buildingTheNetwork(imagesTr) \n",
    "networkSD=buildingTheNetwork(imagesTr) \n",
    "\n",
    "siamNetSD=get_siamese_model(imagesTrSiam) \n",
    "\n",
    "networkSD.summary()\n",
    "networkSR.summary()\n",
    "siamNetSD.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally we fit the models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.6619 - acc: 0.7280     \n",
      "Epoch 2/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.5487 - acc: 0.9170     \n",
      "Epoch 3/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.3653 - acc: 0.9310     \n",
      "Epoch 4/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.2223 - acc: 0.9410     \n",
      "Epoch 5/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.1490 - acc: 0.9510     \n",
      "Epoch 6/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.1163 - acc: 0.9550     \n",
      "Epoch 7/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0770 - acc: 0.9780     \n",
      "Epoch 8/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0590 - acc: 0.9870     \n",
      "Epoch 9/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0575 - acc: 0.9830     \n",
      "Epoch 10/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0362 - acc: 0.9940     \n",
      "Epoch 11/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0252 - acc: 0.9960     \n",
      "Epoch 12/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0177 - acc: 0.9960     \n",
      "Epoch 13/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0125 - acc: 0.9990     \n",
      "Epoch 14/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0131 - acc: 0.9970     \n",
      "Epoch 15/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0125 - acc: 0.9960     \n",
      "Epoch 16/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0079 - acc: 1.0000     \n",
      "Epoch 17/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0045 - acc: 1.0000     \n",
      "Epoch 18/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0056 - acc: 1.0000     \n",
      "Epoch 19/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0042 - acc: 1.0000     \n",
      "Epoch 20/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0032 - acc: 1.0000     \n",
      "Epoch 21/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0024 - acc: 1.0000     \n",
      "Epoch 22/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0041 - acc: 0.9990     \n",
      "Epoch 23/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0020 - acc: 1.0000     \n",
      "Epoch 24/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0029 - acc: 0.9990     \n",
      "Epoch 25/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0017 - acc: 1.0000     \n",
      "Epoch 26/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0015 - acc: 1.0000     \n",
      "Epoch 27/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0023 - acc: 1.0000     \n",
      "Epoch 28/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 29/70\n",
      "1000/1000 [==============================] - 5s - loss: 0.0018 - acc: 0.9990     \n",
      "Epoch 30/70\n",
      "1000/1000 [==============================] - 4s - loss: 8.1022e-04 - acc: 1.0000     \n",
      "Epoch 31/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 32/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0016 - acc: 1.0000     - ETA: 0s - loss: 0.0011 - acc: 1.\n",
      "Epoch 33/70\n",
      "1000/1000 [==============================] - 4s - loss: 9.7847e-04 - acc: 1.0000 \n",
      "Epoch 34/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 35/70\n",
      "1000/1000 [==============================] - 3s - loss: 6.7283e-04 - acc: 1.0000     \n",
      "Epoch 36/70\n",
      "1000/1000 [==============================] - 4s - loss: 6.2556e-04 - acc: 1.0000     \n",
      "Epoch 37/70\n",
      "1000/1000 [==============================] - 4s - loss: 4.6222e-04 - acc: 1.0000     \n",
      "Epoch 38/70\n",
      "1000/1000 [==============================] - 5s - loss: 5.0302e-04 - acc: 1.0000     \n",
      "Epoch 39/70\n",
      "1000/1000 [==============================] - 4s - loss: 5.8116e-04 - acc: 1.0000     \n",
      "Epoch 40/70\n",
      "1000/1000 [==============================] - 3s - loss: 5.5875e-04 - acc: 1.0000     \n",
      "Epoch 41/70\n",
      "1000/1000 [==============================] - 3s - loss: 5.4538e-04 - acc: 1.0000     \n",
      "Epoch 42/70\n",
      "1000/1000 [==============================] - 3s - loss: 4.5542e-04 - acc: 1.0000     \n",
      "Epoch 43/70\n",
      "1000/1000 [==============================] - 3s - loss: 4.9369e-04 - acc: 1.0000     \n",
      "Epoch 44/70\n",
      "1000/1000 [==============================] - 3s - loss: 5.3311e-04 - acc: 1.0000     \n",
      "Epoch 45/70\n",
      "1000/1000 [==============================] - 4s - loss: 3.1368e-04 - acc: 1.0000     \n",
      "Epoch 46/70\n",
      "1000/1000 [==============================] - 4s - loss: 4.0211e-04 - acc: 1.0000     \n",
      "Epoch 47/70\n",
      "1000/1000 [==============================] - 4s - loss: 3.2524e-04 - acc: 1.0000     - ETA: 2s - loss: 2.6552e-04 - \n",
      "Epoch 48/70\n",
      "1000/1000 [==============================] - 4s - loss: 2.3575e-04 - acc: 1.0000     - ETA: 3s - loss: 2.4734e-0\n",
      "Epoch 49/70\n",
      "1000/1000 [==============================] - 4s - loss: 2.7564e-04 - acc: 1.0000     \n",
      "Epoch 50/70\n",
      "1000/1000 [==============================] - 5s - loss: 2.8203e-04 - acc: 1.0000     \n",
      "Epoch 51/70\n",
      "1000/1000 [==============================] - 4s - loss: 5.2601e-04 - acc: 1.0000     \n",
      "Epoch 52/70\n",
      "1000/1000 [==============================] - 4s - loss: 2.4911e-04 - acc: 1.0000     \n",
      "Epoch 53/70\n",
      "1000/1000 [==============================] - 4s - loss: 3.2403e-04 - acc: 1.0000     \n",
      "Epoch 54/70\n",
      "1000/1000 [==============================] - 3s - loss: 5.6374e-04 - acc: 1.0000     - ETA: 0s - loss: 5.9652e-04 - acc: 1.00\n",
      "Epoch 55/70\n",
      "1000/1000 [==============================] - 4s - loss: 1.8858e-04 - acc: 1.0000     \n",
      "Epoch 56/70\n",
      "1000/1000 [==============================] - 3s - loss: 3.7135e-04 - acc: 1.0000     \n",
      "Epoch 57/70\n",
      "1000/1000 [==============================] - 3s - loss: 6.0508e-04 - acc: 1.0000     \n",
      "Epoch 58/70\n",
      "1000/1000 [==============================] - 4s - loss: 2.2206e-04 - acc: 1.0000     \n",
      "Epoch 59/70\n",
      "1000/1000 [==============================] - 3s - loss: 4.4342e-04 - acc: 1.0000     \n",
      "Epoch 60/70\n",
      "1000/1000 [==============================] - 4s - loss: 1.2531e-04 - acc: 1.0000     \n",
      "Epoch 61/70\n",
      "1000/1000 [==============================] - 4s - loss: 5.1468e-04 - acc: 1.0000     \n",
      "Epoch 62/70\n",
      "1000/1000 [==============================] - 4s - loss: 1.2525e-04 - acc: 1.0000     \n",
      "Epoch 63/70\n",
      "1000/1000 [==============================] - 4s - loss: 1.2521e-04 - acc: 1.0000     \n",
      "Epoch 64/70\n",
      "1000/1000 [==============================] - 4s - loss: 9.0835e-05 - acc: 1.0000     \n",
      "Epoch 65/70\n",
      "1000/1000 [==============================] - 3s - loss: 1.0012e-04 - acc: 1.0000     \n",
      "Epoch 66/70\n",
      "1000/1000 [==============================] - 3s - loss: 9.4639e-05 - acc: 1.0000     \n",
      "Epoch 67/70\n",
      "1000/1000 [==============================] - 3s - loss: 9.0502e-05 - acc: 1.0000     \n",
      "Epoch 68/70\n",
      "1000/1000 [==============================] - 3s - loss: 1.1979e-04 - acc: 1.0000     \n",
      "Epoch 69/70\n",
      "1000/1000 [==============================] - 3s - loss: 1.2437e-04 - acc: 1.0000     \n",
      "Epoch 70/70\n",
      "1000/1000 [==============================] - 3s - loss: 7.3851e-05 - acc: 1.0000     \n",
      "Epoch 1/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.6926 - acc: 0.5200     \n",
      "Epoch 2/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.6879 - acc: 0.5690     - ETA: 0s - loss: 0.6872 - acc: 0.5\n",
      "Epoch 3/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.6821 - acc: 0.5830     \n",
      "Epoch 4/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.6776 - acc: 0.5980     \n",
      "Epoch 5/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.6726 - acc: 0.5960     \n",
      "Epoch 6/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.6619 - acc: 0.6330     \n",
      "Epoch 7/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.6538 - acc: 0.6440     \n",
      "Epoch 8/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.6417 - acc: 0.6560     \n",
      "Epoch 9/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.6306 - acc: 0.6730     \n",
      "Epoch 10/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.6152 - acc: 0.7070     \n",
      "Epoch 11/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.5902 - acc: 0.7300     \n",
      "Epoch 12/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.5701 - acc: 0.7180     \n",
      "Epoch 13/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.5390 - acc: 0.7600     \n",
      "Epoch 14/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.5032 - acc: 0.7730     \n",
      "Epoch 15/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 4s - loss: 0.4792 - acc: 0.7750     \n",
      "Epoch 16/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.4445 - acc: 0.7980     \n",
      "Epoch 17/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.4034 - acc: 0.8350     \n",
      "Epoch 18/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.3963 - acc: 0.8310     \n",
      "Epoch 19/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.3716 - acc: 0.8460     \n",
      "Epoch 20/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.3430 - acc: 0.8540     \n",
      "Epoch 21/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.2944 - acc: 0.8880     \n",
      "Epoch 22/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.2833 - acc: 0.8750     \n",
      "Epoch 23/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.2426 - acc: 0.9070     \n",
      "Epoch 24/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.2456 - acc: 0.8920     \n",
      "Epoch 25/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.2009 - acc: 0.9140     \n",
      "Epoch 26/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.2169 - acc: 0.9110     \n",
      "Epoch 27/70\n",
      "1000/1000 [==============================] - 5s - loss: 0.1873 - acc: 0.9270     \n",
      "Epoch 28/70\n",
      "1000/1000 [==============================] - 5s - loss: 0.1871 - acc: 0.9300     \n",
      "Epoch 29/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.1755 - acc: 0.9360     \n",
      "Epoch 30/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.1532 - acc: 0.9450     \n",
      "Epoch 31/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.1464 - acc: 0.9410     - ETA: 0s - loss: 0.1492 - acc: 0\n",
      "Epoch 32/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.1474 - acc: 0.9360     \n",
      "Epoch 33/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.1426 - acc: 0.9440     \n",
      "Epoch 34/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.1173 - acc: 0.9610     \n",
      "Epoch 35/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.1144 - acc: 0.9690     \n",
      "Epoch 36/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.1172 - acc: 0.9610     \n",
      "Epoch 37/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.1037 - acc: 0.9650     \n",
      "Epoch 38/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.1021 - acc: 0.9670     \n",
      "Epoch 39/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0934 - acc: 0.9670     \n",
      "Epoch 40/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0768 - acc: 0.9720     \n",
      "Epoch 41/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0936 - acc: 0.9620     \n",
      "Epoch 42/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0964 - acc: 0.9690     \n",
      "Epoch 43/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0890 - acc: 0.9750     \n",
      "Epoch 44/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0924 - acc: 0.9700     \n",
      "Epoch 45/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0698 - acc: 0.9770     \n",
      "Epoch 46/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0642 - acc: 0.9820     \n",
      "Epoch 47/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0789 - acc: 0.9750     \n",
      "Epoch 48/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0739 - acc: 0.9780     \n",
      "Epoch 49/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0582 - acc: 0.9810     \n",
      "Epoch 50/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0611 - acc: 0.9770     \n",
      "Epoch 51/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0486 - acc: 0.9860     \n",
      "Epoch 52/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0669 - acc: 0.9730     \n",
      "Epoch 53/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0624 - acc: 0.9780     \n",
      "Epoch 54/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0577 - acc: 0.9770     \n",
      "Epoch 55/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0494 - acc: 0.9850     \n",
      "Epoch 56/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0486 - acc: 0.9830     \n",
      "Epoch 57/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0455 - acc: 0.9870     \n",
      "Epoch 58/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0403 - acc: 0.9880     \n",
      "Epoch 59/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0419 - acc: 0.9860     \n",
      "Epoch 60/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0295 - acc: 0.9920     \n",
      "Epoch 61/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0395 - acc: 0.9860     \n",
      "Epoch 62/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0457 - acc: 0.9840     \n",
      "Epoch 63/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0351 - acc: 0.9880     \n",
      "Epoch 64/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0362 - acc: 0.9850     \n",
      "Epoch 65/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0309 - acc: 0.9890     \n",
      "Epoch 66/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0407 - acc: 0.9860     \n",
      "Epoch 67/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0304 - acc: 0.9920     \n",
      "Epoch 68/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0359 - acc: 0.9860     \n",
      "Epoch 69/70\n",
      "1000/1000 [==============================] - 4s - loss: 0.0389 - acc: 0.9850     \n",
      "Epoch 70/70\n",
      "1000/1000 [==============================] - 3s - loss: 0.0376 - acc: 0.9860     \n"
     ]
    }
   ],
   "source": [
    "historySR = networkSR.fit(\n",
    "            imagesTr, SRlabelsTr,\n",
    "            batch_size=50, epochs=70, shuffle=False) \n",
    "\n",
    "historySD = networkSD.fit(\n",
    "            imagesTr, SDlabelsTr,\n",
    "            batch_size=50, epochs=70, shuffle=False) \n",
    "\n",
    "historySiam = siamNetSD.fit(\n",
    "            [imagesTrNewR, imagesTrNewL], [SDlabelsTrSiam],\n",
    "            batch_size=50, epochs=70, validation_split=0.0, validation_data=([imagesValNewR, imagesValNewL], [SDlabelsValSiam]), shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "networkSR.save(\"networkSR.h5\")\n",
    "networkSD.save(\"siamNetSD.h5\")\n",
    "siamNetSD.save(\"siamNetSD.h5\")\n",
    "\n",
    "# networkSR = load_model('networkSR.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.999\n"
     ]
    }
   ],
   "source": [
    "perfSR=[]\n",
    "perfSD=[]\n",
    "predSD = networkSD.predict(imagesTe)\n",
    "predSD=np.around(predSD)\n",
    "resTeSD=np.subtract(SDlabelsTe.flatten(),predSD.flatten())\n",
    "perfSD.append(1-sum(abs(resTeSD))/len(resTeSD))\n",
    "\n",
    "predSR = networkSR.predict(imagesTe)\n",
    "predSR=np.around(predSR)\n",
    "resTeSR=np.subtract(SRlabelsTe.flatten(),predSR.flatten())\n",
    "perfSR.append(1-sum(abs(resTeSR))/len(resTeSR))\n",
    "\n",
    "print(np.mean(perfSD))\n",
    "print(np.mean(perfSR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.954\n"
     ]
    }
   ],
   "source": [
    "perfSDsia=[]\n",
    "predSDsia = siamNetSD.predict([imagesTeNewR,imagesTeNewL])\n",
    "predSDsia=np.around(predSDsia)\n",
    "resTeSDsia=np.subtract(SDlabelsTeSiam.flatten(),predSDsia.flatten())\n",
    "perfSDsia.append(1-sum(abs(resTeSDsia))/len(resTeSDsia))\n",
    "print(np.mean(perfSDsia))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sio.savemat('perfSD10sia.mat', mdict={'perfSDsia':perfSDsia})\n",
    "\n",
    "historyAccSDsia=historySiam.history['acc']\n",
    "\n",
    "sio.savemat('historyAccSDsia10.mat', mdict={'historyAccSDsia':historyAccSDsia})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x24d026d8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networkSR.layers[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-2717bdb70f1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "def grad_cam(x, model, sess, predicted_class, layer_name, nb_classes):\n",
    "    print(\"Setting gradients to 1 for target class and rest to 0\")\n",
    "    # Conv layer tensor [?,7,7,512]\n",
    "    conv_layer = model.layers[18]\n",
    "    # [1000]-D tensor with target class index set to 1 and rest as 0\n",
    "    one_hot = tf.sparse_to_dense(predicted_class, [nb_classes], 1.0)\n",
    "    signal = tf.mul(vgg.layers['fc3'], one_hot)\n",
    "    loss = tf.reduce_mean(signal)\n",
    "\n",
    "    grads = tf.gradients(loss, conv_layer)[0]\n",
    "    \n",
    "# Normalizing the gradients\n",
    "    norm_grads = tf.div(grads, tf.sqrt(tf.reduce_mean(tf.square(grads))) + tf.constant(1e-5))\n",
    "\n",
    "    output, grads_val = sess.run([conv_layer, norm_grads], feed_dict={vgg.imgs: x})\n",
    "    output = output[0]           # [7,7,512]\n",
    "    grads_val = grads_val[0]\t # [7,7,512]\n",
    "\n",
    "    weights = np.mean(grads_val, axis = (0, 1)) \t\t\t# [512]\n",
    "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\t# [7,7]\n",
    "\n",
    "    # Taking a weighted average\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * output[:, :, i]\n",
    "\n",
    "# Passing through ReLU\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "    cam = resize(cam, (224,224))\n",
    "\n",
    "    # Converting grayscale to 3-D\n",
    "    cam3 = np.expand_dims(cam, axis=2)\n",
    "    cam3 = np.tile(cam3,[1,1,3])\n",
    "\n",
    "    return cam3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import imread, imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_cam(x, vgg, sess, predicted_class, layer_name, nb_classes):\n",
    "    print(\"Setting gradients to 1 for target class and rest to 0\")\n",
    "    \n",
    "    # Conv layer tensor [?,7,7,512]\n",
    "    conv_layer = vgg.layers[layer_name]\n",
    "    \n",
    "    # [1000]-D tensor with target class index set to 1 and rest as 0\n",
    "    one_hot = tf.sparse_to_dense(predicted_class, [nb_classes], 1.0)\n",
    "    signal = tf.mul(vgg.layers['fc3'], one_hot)\n",
    "    loss = tf.reduce_mean(signal)\n",
    "\n",
    "    grads = tf.gradients(loss, conv_layer)[0]\n",
    "    \n",
    "# Normalizing the gradients\n",
    "    norm_grads = tf.div(grads, tf.sqrt(tf.reduce_mean(tf.square(grads))) + tf.constant(1e-5))\n",
    "\n",
    "    output, grads_val = sess.run([conv_layer, norm_grads], feed_dict={vgg.imgs: x})\n",
    "    output = output[0]           # [7,7,512]\n",
    "    grads_val = grads_val[0]\t # [7,7,512]\n",
    "\n",
    "    weights = np.mean(grads_val, axis = (0, 1)) \t\t\t# [512]\n",
    "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\t# [7,7]\n",
    "\n",
    "    # Taking a weighted average\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * output[:, :, i]\n",
    "\n",
    "# Passing through ReLU\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "    cam = resize(cam, (224,224))\n",
    "\n",
    "    # Converting grayscale to 3-D\n",
    "    cam3 = np.expand_dims(cam, axis=2)\n",
    "    cam3 = np.tile(cam3,[1,1,3])\n",
    "\n",
    "    return cam3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
